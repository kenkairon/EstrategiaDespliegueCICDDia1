# README: ConfiguraciÃ³n de CI/CD para Apache Airflow con GitHub Actions

## ğŸ“‹ GuÃ­a Paso a Paso para Principiantes

Este tutorial te guiarÃ¡ para configurar un pipeline CI/CD completo para tus DAGs de Apache Airflow usando GitHub Actions.

---

## ğŸ¯ Â¿QuÃ© vamos a lograr?

- âœ… Tests automÃ¡ticos cada vez que hagas push
- âœ… ValidaciÃ³n de sintaxis de DAGs
- âœ… Despliegue automÃ¡tico a diferentes ambientes
- âœ… Reporte de cobertura de cÃ³digo

---

## ğŸ“ Estructura del Proyecto

Primero, crea esta estructura en tu repositorio:

```
mi-proyecto-airflow/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci-cd-airflow.yml
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_pipeline.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ test_etl_pipeline.py
â”‚       â””â”€â”€ test_dag_integration.py
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ deploy.sh
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ PASO 1: Preparar tu Repositorio

### 1.1 Crear el repositorio en GitHub

1. Ve a GitHub.com
2. Click en "New repository"
3. Nombre: `airflow-cicd-tutorial`
4. Marca "Add a README file"
5. Click "Create repository"

### 1.2 Clonar el repositorio localmente

```bash
git clone https://github.com/TU-USUARIO/airflow-cicd-tutorial.git
cd airflow-cicd-tutorial
```

---

## ğŸ“ PASO 2: Crear el DAG de Ejemplo

### 2.1 Crear la carpeta de DAGs

```bash
mkdir -p dags
```

### 2.2 Crear tu primer DAG

Crea el archivo `dags/etl_pipeline.py`:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

# ConfiguraciÃ³n por defecto
default_args = {
    'owner': 'tu-nombre',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# Definir el DAG
dag = DAG(
    'etl_pipeline',
    default_args=default_args,
    description='Pipeline ETL de ejemplo para CI/CD',
    schedule_interval='@daily',
    catchup=False,
    tags=['example', 'etl'],
)

# Funciones de las tareas
def extract_data():
    print("ğŸ“¥ Extrayendo datos...")
    return {"status": "success", "records": 100}

def transform_data():
    print("ğŸ”„ Transformando datos...")
    return {"status": "transformed", "records": 95}

def load_data():
    print("ğŸ“¤ Cargando datos...")
    return {"status": "loaded"}

# Definir tareas
start = BashOperator(
    task_id='start',
    bash_command='echo "Iniciando pipeline ETL"',
    dag=dag,
)

extract = PythonOperator(
    task_id='extract',
    python_callable=extract_data,
    dag=dag,
)

transform = PythonOperator(
    task_id='transform',
    python_callable=transform_data,
    dag=dag,
)

load = PythonOperator(
    task_id='load',
    python_callable=load_data,
    dag=dag,
)

end = BashOperator(
    task_id='end',
    bash_command='echo "âœ… Pipeline completado"',
    dag=dag,
)

# Definir dependencias
start >> extract >> transform >> load >> end
```

---

## ğŸ§ª PASO 3: Crear los Tests

### 3.1 Crear estructura de tests

```bash
mkdir -p tests/dags
```

### 3.2 Crear archivo de tests

Crea `tests/dags/test_etl_pipeline.py`:

```python
import pytest
from airflow.models import DagBag
from datetime import datetime, timedelta

class TestETLPipeline:
    
    @pytest.fixture(scope='class')
    def dagbag(self):
        """Cargar DAGs una vez por clase de tests"""
        return DagBag(dag_folder='dags/', include_examples=False)
    
    def test_dag_loaded(self, dagbag):
        """Verificar que el DAG se carga correctamente"""
        dag = dagbag.get_dag('etl_pipeline')
        assert dag is not None, "âŒ DAG etl_pipeline no encontrado"
        print("âœ… DAG cargado correctamente")
    
    def test_dag_has_tasks(self, dagbag):
        """Verificar que el DAG tiene tareas"""
        dag = dagbag.get_dag('etl_pipeline')
        assert len(dag.tasks) >= 5, f"âŒ DAG tiene solo {len(dag.tasks)} tareas"
        print(f"âœ… DAG tiene {len(dag.tasks)} tareas")
    
    def test_task_dependencies(self, dagbag):
        """Verificar dependencias entre tareas"""
        dag = dagbag.get_dag('etl_pipeline')
        
        extract = dag.get_task('extract')
        transform = dag.get_task('transform')
        load = dag.get_task('load')
        
        assert transform in extract.downstream_list, "âŒ Transform no depende de extract"
        assert load in transform.downstream_list, "âŒ Load no depende de transform"
        print("âœ… Dependencias correctas")
    
    def test_dag_schedule(self, dagbag):
        """Verificar configuraciÃ³n de scheduling"""
        dag = dagbag.get_dag('etl_pipeline')
        assert dag.schedule_interval is not None, "âŒ DAG sin schedule"
        print(f"âœ… Schedule: {dag.schedule_interval}")
```

Crea `tests/dags/test_dag_integration.py`:

```python
import pytest
from airflow.models import DagBag

class TestDAGIntegration:
    
    @pytest.fixture(scope='class')
    def dagbag(self):
        return DagBag(dag_folder='dags/', include_examples=False)
    
    def test_no_import_errors(self, dagbag):
        """Verificar que no hay errores de importaciÃ³n"""
        assert not dagbag.import_errors, f"âŒ Errores: {dagbag.import_errors}"
        print("âœ… Sin errores de importaciÃ³n")
    
    def test_dag_no_cycles(self, dagbag):
        """Verificar que no hay ciclos en dependencias"""
        dag = dagbag.get_dag('etl_pipeline')
        # El mÃ©todo test_cycle() devuelve False si NO hay ciclos
        assert dag.test_cycle() == False, "âŒ DAG tiene ciclos"
        print("âœ… Sin ciclos de dependencias")
```

### 3.3 Crear archivo de requirements

Crea `requirements.txt`:

```
apache-airflow==2.7.0
pytest==7.4.3
pytest-cov==4.1.0
```

---

## âš™ï¸ PASO 4: Configurar GitHub Actions

### 4.1 Crear carpeta de workflows

```bash
mkdir -p .github/workflows
```

### 4.2 Crear el workflow de CI/CD

Crea `.github/workflows/ci-cd-airflow.yml`:

```yaml
name: Airflow CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  AIRFLOW_VERSION: 2.7.0
  PYTHON_VERSION: '3.9'

jobs:
  test:
    name: ğŸ§ª Test DAGs
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ Checkout cÃ³digo
      uses: actions/checkout@v3
    
    - name: ğŸ Configurar Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: ğŸ“¦ Instalar dependencias
      run: |
        pip install --upgrade pip
        pip install apache-airflow==${{ env.AIRFLOW_VERSION }}
        pip install pytest pytest-cov
    
    - name: âœ… Validar sintaxis de DAGs
      run: |
        python -c "
        from airflow.models import DagBag
        dagbag = DagBag(dag_folder='dags/', include_examples=False)
        if dagbag.import_errors:
            print('âŒ ERRORES EN DAGs:', dagbag.import_errors)
            exit(1)
        print(f'âœ… {len(dagbag.dags)} DAGs cargados correctamente')
        "
    
    - name: ğŸ§ª Ejecutar tests
      run: |
        pytest tests/dags/ -v --cov=dags --cov-report=xml --cov-report=term
    
    - name: ğŸ“Š Subir reporte de cobertura
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  deploy-dev:
    name: ğŸš€ Deploy a Dev
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    
    steps:
    - name: ğŸ“¥ Checkout cÃ³digo
      uses: actions/checkout@v3
    
    - name: ğŸš€ Deploy a Development
      run: |
        echo "ğŸ”§ Desplegando a ambiente de desarrollo..."
        echo "âœ… Deployment exitoso"

  deploy-prod:
    name: ğŸ¯ Deploy a ProducciÃ³n
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - name: ğŸ“¥ Checkout cÃ³digo
      uses: actions/checkout@v3
    
    - name: ğŸ¯ Deploy a Production
      run: |
        echo "ğŸš€ Desplegando a producciÃ³n..."
        echo "âœ… Deployment a producciÃ³n exitoso"
```

---

## ğŸ”„ PASO 5: Subir CÃ³digo a GitHub

```bash
# Agregar todos los archivos
git add .

# Crear commit
git commit -m "ConfiguraciÃ³n inicial de CI/CD para Airflow"

# Subir a GitHub
git push origin main
```

---

## ğŸ‘€ PASO 6: Verificar que Funciona

### 6.1 Ver el workflow en acciÃ³n

1. Ve a tu repositorio en GitHub
2. Click en la pestaÃ±a "Actions"
3. DeberÃ­as ver tu workflow ejecutÃ¡ndose
4. Click en el workflow para ver los detalles

### 6.2 Verificar que los tests pasan

En la secciÃ³n "Jobs" deberÃ­as ver:
- âœ… Test DAGs (verde si todo estÃ¡ bien)
- âœ… Deploy a ProducciÃ³n (si hiciste push a main)

---

## ğŸ§ª PASO 7: Probar Localmente (Opcional)

### 7.1 Instalar dependencias

```bash
pip install -r requirements.txt
```

### 7.2 Ejecutar tests

```bash
# Ejecutar todos los tests
pytest tests/dags/ -v

# Ejecutar con cobertura
pytest tests/dags/ -v --cov=dags --cov-report=term
```

### 7.3 Validar DAGs manualmente

```bash
python -c "
from airflow.models import DagBag
dagbag = DagBag(dag_folder='dags/', include_examples=False)
print(f'DAGs encontrados: {len(dagbag.dags)}')
for dag_id in dagbag.dag_ids:
    print(f'  - {dag_id}')
"
```

---

## ğŸŒ¿ PASO 8: Trabajar con Branches

### 8.1 Crear rama de desarrollo

```bash
# Crear y cambiar a rama develop
git checkout -b develop

# Hacer cambios en tu DAG
# ... editar archivos ...

# Commit y push
git add .
git commit -m "Nuevas features en DAG"
git push origin develop
```

Esto activarÃ¡ el workflow y desplegarÃ¡ a "dev" automÃ¡ticamente.

### 8.2 Crear Pull Request

1. Ve a GitHub
2. Click en "Pull requests"
3. Click "New pull request"
4. Selecciona `develop` â†’ `main`
5. Los tests se ejecutarÃ¡n automÃ¡ticamente
6. Si pasan, puedes hacer merge

---

## ğŸ“ Conceptos Clave

### Â¿QuÃ© es CI/CD?

- **CI (Continuous Integration)**: Tests automÃ¡ticos al hacer cambios
- **CD (Continuous Deployment)**: Despliegue automÃ¡tico cuando los tests pasan

### Â¿Por quÃ© es importante para DAGs?

1. **Prevenir errores**: Detecta problemas antes de producciÃ³n
2. **Confianza**: Sabes que tu cÃ³digo funciona
3. **Rapidez**: Deploy automÃ¡tico sin pasos manuales
4. **Trazabilidad**: Historial de todos los cambios

---

## ğŸ”§ Troubleshooting

### Error: "DAG not found"

```bash
# Verifica que la estructura sea correcta
ls -la dags/
# Debe mostrar etl_pipeline.py
```

### Error: "Module not found"

```bash
# Reinstala dependencias
pip install -r requirements.txt
```

### El workflow no se ejecuta

- Verifica que el archivo estÃ© en `.github/workflows/`
- Verifica que hiciste push a `main` o `develop`
- Revisa la pestaÃ±a "Actions" en GitHub

---

## ğŸ“š PrÃ³ximos Pasos

1. âœ… Agrega mÃ¡s DAGs a la carpeta `dags/`
2. âœ… Crea mÃ¡s tests en `tests/dags/`
3. âœ… Configura ambientes reales (Docker, Kubernetes)
4. âœ… Agrega notificaciones (Slack, Email)
5. âœ… Implementa mÃ©tricas y monitoring

---

## ğŸ¤ Â¿Necesitas Ayuda?

- Revisa los logs en la pestaÃ±a "Actions" de GitHub
- Cada step muestra output detallado
- Los errores aparecen en rojo con descripciones

---

## âœ… Checklist Final

- [ ] Repositorio creado en GitHub
- [ ] Estructura de carpetas correcta
- [ ] DAG creado en `dags/`
- [ ] Tests creados en `tests/dags/`
- [ ] Workflow en `.github/workflows/`
- [ ] CÃ³digo subido a GitHub
- [ ] Workflow ejecutÃ¡ndose correctamente
- [ ] Tests pasando (verde en Actions)

